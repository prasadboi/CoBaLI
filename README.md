# CoBaLI: Continuous Batching and Prefill Splitting for LLM Inference

A GPU programming course project demonstrating continuous batching and prefill splitting optimizations for large language model inference.

## ğŸ¯ Project Goals

Starting with a small open-source LLM (Qwen 0.5B in GGUF format), this project implements:

1. **Phase 1**: Baseline sequential inference (using llama.cpp)
2. **Phase 2**: Continuous batching scheduler (C++)
3. **Phase 3**: Prefill splitting/chunking (C++)
4. **Phase 4**: Custom CUDA kernels (future)

**Key Constraint**: No use of advanced libraries like vLLM or TensorRT-LLM â€” everything implemented manually using llama.cpp as baseline.

## ğŸ“Š Expected Performance Improvements

| Phase | Optimization | Expected Speedup |
|-------|--------------|------------------|
| 1 | Baseline | 1x (reference) |
| 2 | Continuous Batching | 2-3x throughput |
| 3 | + Prefill Splitting | 5-10x throughput |
| 4 | + Custom CUDA Kernels | 10-15x throughput |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Layer (Orchestration & Benchmarking)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ C++ Layer (Scheduling & Batch Management)                   â”‚
â”‚  - ContinuousBatcher: Dynamic batch formation               â”‚
â”‚  - PrefillSplitter: Chunked prefill scheduling              â”‚
â”‚  - KVCacheManager: GPU memory management                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ CUDA Layer (GPU Execution)                                  â”‚
â”‚  - llama.cpp kernels (Phase 1-3)                           â”‚
â”‚  - Custom CUDA kernels (Phase 4)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **OS**: Linux
- **GPU**: NVIDIA RTX 2080 Ti or better (compute capability 7.5+)
- **CUDA**: 12.4 or 13.0
- **CMake**: 3.18+
- **C++**: C++17 compiler (GCC 9+ or Clang 10+)
- **Python**: 3.8+ (optional, for benchmarking)

### Installation

```bash
# Clone repository
git clone <your-repo-url>
cd CoBaLI

# Run setup script (builds everything)
./scripts/setup.sh

# Download model (Qwen 0.5B, ~300MB)
./scripts/download_model.sh
```

### Running

```bash
# Phase 1: Baseline (sequential)
./build/cobali_main baseline models/qwen2-0_5b-instruct-q4_0.gguf

# Phase 2: Continuous batching
./build/cobali_main batching models/qwen2-0_5b-instruct-q4_0.gguf

# Phase 3: Full CoBaLI (batching + prefill splitting)
./build/cobali_main full models/qwen2-0_5b-instruct-q4_0.gguf
```

### Running Examples

```bash
# Baseline sequential inference
./build/examples/example_baseline models/qwen2-0_5b-instruct-q4_0.gguf

# Continuous batching demo
./build/examples/example_continuous_batching models/qwen2-0_5b-instruct-q4_0.gguf

# Full CoBaLI demo
./build/examples/example_full_cobali models/qwen2-0_5b-instruct-q4_0.gguf
```

## ğŸ“ Repository Structure

```
cobali/
â”œâ”€â”€ include/cobali/          # C++ header files
â”‚   â”œâ”€â”€ common/              # Types, config, utilities
â”‚   â”œâ”€â”€ scheduler/           # Batching and splitting logic
â”‚   â”œâ”€â”€ memory/              # KV cache management
â”‚   â”œâ”€â”€ baseline/            # Sequential engine
â”‚   â”œâ”€â”€ engine/              # Main inference engine
â”‚   â””â”€â”€ kernels/             # CUDA kernel headers
â”‚
â”œâ”€â”€ src/                     # C++ implementation
â”‚   â”œâ”€â”€ scheduler/           # YOUR CORE IMPLEMENTATIONS
â”‚   â”‚   â”œâ”€â”€ continuous_batcher.cpp
â”‚   â”‚   â””â”€â”€ prefill_splitter.cpp
â”‚   â”œâ”€â”€ kernels/             # YOUR CUDA KERNELS (Phase 4)
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ examples/cpp/            # Example programs
â”œâ”€â”€ tests/cpp/               # C++ unit tests
â”œâ”€â”€ benchmarks/              # Python benchmarking scripts
â”œâ”€â”€ docs/                    # Detailed documentation
â”œâ”€â”€ configs/                 # Configuration files
â””â”€â”€ scripts/                 # Setup and build scripts
```

## ğŸ“– Documentation

- [01_design_overview.md](docs/01_design_overview.md) - Architecture and design
- [02_baseline_implementation.md](docs/02_baseline_implementation.md) - Phase 1: Baseline
- [03_continuous_batching.md](docs/03_continuous_batching.md) - Phase 2: Batching
- [04_prefill_splitting.md](docs/04_prefill_splitting.md) - Phase 3: Splitting
- [05_cuda_kernels.md](docs/05_cuda_kernels.md) - Phase 4: Custom kernels (TODO)
- [06_results_analysis.md](docs/06_results_analysis.md) - Benchmarks and results (TODO)

## ğŸ”§ Configuration

Configuration files in `configs/`:

```yaml
# configs/cobali_config.yaml
batching:
  enable_continuous_batching: true
  max_batch_size: 32
  max_tokens_per_batch: 4096

prefill_splitting:
  enabled: true
  chunk_size: 512
  decode_priority_weight: 0.7  # 0.0-1.0
```

## ğŸ§ª Testing

```bash
# Build tests
cd build
cmake --build . --target test_request_queue

# Run tests
./test_request_queue
./test_batch_manager
./test_prefill_splitter
```

## ğŸ“Š Benchmarking

```bash
# Python benchmarks (after setup)
source venv/bin/activate
python benchmarks/run_baseline.py
python benchmarks/run_continuous_batch.py
python benchmarks/compare_all.py
```

## ğŸ“ Academic Context

This project is for a GPU programming course and focuses on:

1. **Host-side optimization**: C++ scheduling algorithms
2. **GPU memory management**: KV cache allocation
3. **Batched execution**: Efficient GPU utilization
4. **Custom CUDA kernels**: Low-level GPU programming

## ğŸ” Key Concepts

### Continuous Batching
Instead of waiting for all requests in a batch to complete:
- Dynamically add new requests to active batch
- Remove completed requests mid-execution
- Maximize GPU utilization

### Prefill Splitting
Break large prompt processing into chunks:
- Process 512 tokens at a time (configurable)
- Interleave with decode steps from other requests
- Improves fairness and reduces time-to-first-token

### KV Cache Management
Per-request KV cache allocation:
- Each request gets separate GPU memory slot
- Dynamic allocation/deallocation
- Enables concurrent request processing

## ğŸ“ License

[Your License Here]

## ğŸ™ Acknowledgments

- [llama.cpp](https://github.com/ggerganov/llama.cpp) for baseline implementation
- [Qwen](https://github.com/QwenLM/Qwen) for the model
- [Orca/vLLM paper](https://arxiv.org/abs/2309.06180) for continuous batching inspiration

## ğŸ“® Contact

[Your contact information]

---

**Note**: This is an academic project for learning GPU programming. For production use, consider vLLM, TensorRT-LLM, or other mature inference frameworks.
